{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#install required libraries"
      ],
      "metadata": {
        "id": "f5-dcp7LBjNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXVHhat2BeIb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "from typing import List, Dict, Any, Tuple\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from openai import OpenAI\n",
        "import time\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import seaborn as sns\n",
        "from dotenv import load_dotenv\n",
        "import openai\n",
        "import os\n",
        "from langchain_community.retrievers import TavilySearchAPIRetriever"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "load_dotenv()\n",
        "\n",
        "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "tavily_api_key = os.getenv(\"TAVILY_API_KEY\")"
      ],
      "metadata": {
        "id": "pRHBBBNvBg2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_qa = pd.read_csv('qna_dataset.csv')\n",
        "df_qa = df_qa.sample(500, random_state=0).reset_index(drop=True)\n",
        "df_qa.head()"
      ],
      "metadata": {
        "id": "SOrDMPTiBpuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#combine question and answer into one column\n",
        "df_qa['combined_column']=(\n",
        "    \"Question : \"+df_qa['Question'].astype('str') + '. ' +\n",
        "    \" Answer : \"+df_qa['Answer'].astype('str') + '. ' +\n",
        "    \"Type : \" + df_qa['qtype'].astype('str') + '. '\n",
        ")\n",
        "df_qa.head()"
      ],
      "metadata": {
        "id": "6EeNahpzBpsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_md = pd.read_csv('medical_device_manuals_dataset.csv')\n",
        "df_md = df_md.sample(500, random_state=0).reset_index(drop=True)\n",
        "df_md.head()"
      ],
      "metadata": {
        "id": "ah1qc4OZBppo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_md['combined_column']= (\n",
        "    \"Device Name: \" + df_md['Device_Name'].astype('str') + '. ' +\n",
        "    \"Model: \" + df_md['Model_Number'].astype('str') + '. ' +\n",
        "    \"Manufacturer: \" + df_md['Manufacturer'].astype('str') + '. ' +\n",
        "    \"Indications: \"+ df_md['Indications_for_Use'].astype('str') + '. ' +\n",
        "    \"Contraindications: \"+ df_md['Contraindications'].fillna('None').astype('str')\n",
        ")\n",
        "df_md.head()"
      ],
      "metadata": {
        "id": "dhR67rRPBpko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_md['combined_column'] = (\n",
        "    df_md['combined_column']\n",
        "    .fillna(\"\")\n",
        "    .astype(str)\n",
        "    .str.strip()\n",
        "    )"
      ],
      "metadata": {
        "id": "gUr-gIwSBwCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "\n",
        "client = chromadb.PersistentClient(path=\"./chroma_db\")"
      ],
      "metadata": {
        "id": "VLbNUm-UBxmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collection1 = client.get_or_create_collection(name=\"medical_qna\")"
      ],
      "metadata": {
        "id": "5Om6eXW4ByRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collection1.add(\n",
        "    documents = df_qa['combined_column'].tolist(),\n",
        "    metadatas = df_qa.to_dict(orient='records'),\n",
        "    ids=df_qa.index.astype(str).tolist()\n",
        ")\n",
        "print(\"\\n Medical Q&A collection created and data added.\")"
      ],
      "metadata": {
        "id": "4tT5gtHOBzz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collection2 = client.get_or_create_collection(name=\"medical_device_manual\")"
      ],
      "metadata": {
        "id": "-edqlr2dB0sp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collection2.add(\n",
        "    documents = df_md['combined_column'].tolist(),\n",
        "    metadatas = df_md.to_dict(orient='records'),\n",
        "    ids=df_md.index.astype(str).tolist()\n",
        ")\n",
        "print(\"\\n Medical Device Manual collection created and data added.\")"
      ],
      "metadata": {
        "id": "KH9cbggSB2no"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What are devices used in surgery\"\n",
        "results = collection2.query(query_texts=[query], n_results=5)\n",
        "print(\"\\nQuery Results:\", results)"
      ],
      "metadata": {
        "id": "qUKuikEoB2lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.retrievers import TavilySearchAPIRetriever\n",
        "# Initialize retriever\n",
        "retriever = TavilySearchAPIRetriever(\n",
        "    api_key=os.getenv(\"TAVILY_API_KEY\"),\n",
        "    k=4 )\n",
        "query = \"What is the speciality of Momento\"\n",
        "# Use invoke() correctly\n",
        "results = retriever.invoke(query)\n",
        "results"
      ],
      "metadata": {
        "id": "7WcDe04KB2jG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"What is the speciality of Dunkirk?\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n",
        "\n",
        "def call_llm(state):\n",
        "    prompt = state[\"promt\"]\n",
        "    response = get_llm_response(prompt)\n",
        "    state[\"response\"] = response\n",
        "    return state\n"
      ],
      "metadata": {
        "id": "iG-snAsRB2gP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_llm_response(prompt: str) -> str:\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"llama-3.3-70b-versatile\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    return response.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "M9yO5M10B80Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, Dict\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "# 1. STATE SCHEMA (must be defined BEFORE node functions)\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    query: str\n",
        "    context: str\n",
        "    promt: str\n",
        "    response: str\n",
        "    source: str\n",
        "    is_relevant: str\n",
        "    iteration_count: int\n",
        "\n",
        "\n",
        "# 2. NODE FUNCTIONS\n",
        "\n",
        "def retrieve_context_qna(state: Dict) -> Dict:\n",
        "    print(\"\\nRetrieving context for Q&A...\")\n",
        "    query = state[\"query\"]\n",
        "\n",
        "    results = collection1.query(query_texts=[query], n_results=3)\n",
        "    raw_docs = results.get(\"documents\", [[]])[0]\n",
        "    safe_docs = [str(x) for x in raw_docs if x is not None]\n",
        "    context = \"\\n\".join(safe_docs)\n",
        "\n",
        "    state[\"context\"] = context\n",
        "    return state\n",
        "\n",
        "\n",
        "def retrieve_context_md(state: Dict) -> Dict:\n",
        "    print(\"\\nRetrieving context for Medical Device Manual...\")\n",
        "    query = state[\"query\"]\n",
        "\n",
        "    results = collection2.query(query_texts=[query], n_results=3)\n",
        "    raw_docs = results.get(\"documents\", [[]])[0]\n",
        "    safe_docs = [str(x) for x in raw_docs if x is not None]\n",
        "    context = \"\\n\".join(safe_docs)\n",
        "\n",
        "    state[\"context\"] = context\n",
        "    return state\n",
        "\n",
        "\n",
        "def tavily_web_search(state: Dict) -> Dict:\n",
        "    print(\"\\nPerforming Tavily web search...\")\n",
        "    query = state[\"query\"]\n",
        "\n",
        "    results = retriever.invoke(query)\n",
        "    context = \"\\n\".join(str(doc.page_content) for doc in results)\n",
        "\n",
        "    state[\"context\"] = context\n",
        "    return state\n",
        "\n",
        "\n",
        "def router(state: Dict) -> Dict:\n",
        "    query = state[\"query\"]\n",
        "\n",
        "    decision_prompt = f\"\"\"\n",
        "You are a routing agent. Based on the user query, choose exactly one:\n",
        "- retrieve qna\n",
        "- retrieve device\n",
        "- web search\n",
        "\n",
        "Query: {query}\n",
        "\n",
        "Respond with exactly one of:\n",
        "retrieve qna\n",
        "retrieve device\n",
        "web search\n",
        "\"\"\"\n",
        "\n",
        "    decision = get_llm_response(decision_prompt).strip().lower().replace(\".\", \"\")\n",
        "    print(\"\\nRouter decision:\", decision)\n",
        "\n",
        "    state[\"source\"] = decision\n",
        "    return state\n",
        "\n",
        "\n",
        "def route_decision(state: Dict) -> str:\n",
        "    return state[\"source\"]\n",
        "\n",
        "\n",
        "def check_relevance(state: Dict) -> Dict:\n",
        "    print(\"\\nChecking relevance of retrieved context...\")\n",
        "    query = state[\"query\"]\n",
        "    context = state[\"context\"]\n",
        "\n",
        "    relevance_prompt = f\"\"\"\n",
        "Check if the context is relevant to the query. Respond with only 'Yes' or 'No'.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Query:\n",
        "{query}\n",
        "\"\"\"\n",
        "\n",
        "    decision = get_llm_response(relevance_prompt).strip()\n",
        "    print(\"Relevance decision:\", decision)\n",
        "\n",
        "    state[\"is_relevant\"] = decision\n",
        "    return state\n",
        "\n",
        "\n",
        "def relevance_decision(state: Dict) -> str:\n",
        "    count = state.get(\"iteration_count\", 0) + 1\n",
        "    state[\"iteration_count\"] = count\n",
        "\n",
        "    if count >= 3:\n",
        "        print(\"\\nMax iterations reached. Forcing Yes.\")\n",
        "        state[\"is_relevant\"] = \"Yes\"\n",
        "\n",
        "    return state[\"is_relevant\"]\n",
        "\n",
        "\n",
        "def build_prompt(state: Dict) -> Dict:\n",
        "    query = state[\"query\"]\n",
        "    context = state[\"context\"]\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are a medical assistant AI. Use ONLY the context below to answer the user's question.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "User Query:\n",
        "{query}\n",
        "\n",
        "Provide a clear, concise answer.\n",
        "\"\"\"\n",
        "\n",
        "    state[\"promt\"] = prompt\n",
        "    return state\n",
        "\n",
        "\n",
        "def call_llm(state: Dict) -> Dict:\n",
        "    response = get_llm_response(state[\"promt\"])\n",
        "    state[\"response\"] = response\n",
        "    return state\n",
        "\n",
        "\n",
        "# 3. BUILD GRAPH\n",
        "\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "workflow.add_node(\"router\", router)\n",
        "workflow.add_node(\"Retrieve_QnA\", retrieve_context_qna)\n",
        "workflow.add_node(\"Retrieve_Device\", retrieve_context_md)\n",
        "workflow.add_node(\"Web_Search\", tavily_web_search)\n",
        "workflow.add_node(\"Check_Relevance\", check_relevance)\n",
        "workflow.add_node(\"Relevance_Decision\", relevance_decision)\n",
        "workflow.add_node(\"Augment\", build_prompt)\n",
        "workflow.add_node(\"Generate\", call_llm)\n",
        "\n",
        "workflow.add_edge(START, \"router\")\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"router\",\n",
        "    route_decision,\n",
        "    {\n",
        "        \"retrieve qna\": \"Retrieve_QnA\",\n",
        "        \"retrieve device\": \"Retrieve_Device\",\n",
        "        \"web search\": \"Web_Search\",\n",
        "    }\n",
        ")\n",
        "\n",
        "workflow.add_edge(\"Retrieve_QnA\", \"Check_Relevance\")\n",
        "workflow.add_edge(\"Retrieve_Device\", \"Check_Relevance\")\n",
        "workflow.add_edge(\"Web_Search\", \"Check_Relevance\")\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"Check_Relevance\",\n",
        "    relevance_decision,\n",
        "    {\n",
        "        \"Yes\": \"Augment\",\n",
        "        \"No\": \"Web_Search\",\n",
        "    }\n",
        ")\n",
        "\n",
        "workflow.add_edge(\"Augment\", \"Generate\")\n",
        "workflow.add_edge(\"Generate\", END)\n",
        "\n",
        "agentic_rag = workflow.compile()\n"
      ],
      "metadata": {
        "id": "jZ7z7MBJB8yD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_prompt(state):\n",
        "    \"\"\"Combine query + retrieved context into a final LLM prompt.\"\"\"\n",
        "    query = state[\"query\"]\n",
        "    context = state[\"context\"]\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are a medical assistant AI. Use ONLY the context below to answer the user's question.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "User Query:\n",
        "{query}\n",
        "\n",
        "Provide a clear, concise answer.\n",
        "\"\"\"\n",
        "    state[\"promt\"] = prompt\n",
        "    return state\n"
      ],
      "metadata": {
        "id": "-vPUsgG7B8vt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, Literal\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    query: str\n",
        "    context: str\n",
        "    promt: str\n",
        "    response: str\n",
        "    source: str\n",
        "    is_relevant: str\n",
        "    iteration_count: int\n",
        "\n",
        "# Define input_state BEFORE using it\n",
        "input_state = {\n",
        "    \"query\": \"What is the treatment for cancer?\",\n",
        "    \"context\": \"\",\n",
        "    \"promt\": \"\",\n",
        "    \"response\": \"\",\n",
        "    \"source\": \"\",\n",
        "    \"is_relevant\": \"\",\n",
        "    \"iteration_count\": 0\n",
        "}\n",
        "\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Register nodes\n",
        "workflow.add_node(\"router\", router)\n",
        "workflow.add_node(\"Retrieve_QnA\", retrieve_context_qna)\n",
        "workflow.add_node(\"Retrieve_Device\", retrieve_context_md)\n",
        "workflow.add_node(\"Web_Search\", tavily_web_search)\n",
        "workflow.add_node(\"Check_Relevance\", check_relevance)\n",
        "workflow.add_node(\"Relevance_Decision\", relevance_decision)\n",
        "workflow.add_node(\"Augment\", build_prompt)\n",
        "workflow.add_node(\"Generate\", call_llm)\n",
        "\n",
        "# Start → Router\n",
        "workflow.add_edge(START, \"router\")\n",
        "\n",
        "# Router → Retrieval Nodes\n",
        "workflow.add_conditional_edges(\n",
        "    \"router\",\n",
        "    route_decision,\n",
        "    {\n",
        "        \"retrieve qna\": \"Retrieve_QnA\",\n",
        "        \"retrieve device\": \"Retrieve_Device\",\n",
        "        \"web search\": \"Web_Search\",\n",
        "    }\n",
        ")\n",
        "\n",
        "# Retrieval → Relevance Check\n",
        "workflow.add_edge(\"Retrieve_QnA\", \"Check_Relevance\")\n",
        "workflow.add_edge(\"Retrieve_Device\", \"Check_Relevance\")\n",
        "workflow.add_edge(\"Web_Search\", \"Check_Relevance\")\n",
        "\n",
        "# Relevance Check → Augment or Retry\n",
        "workflow.add_conditional_edges(\n",
        "    \"Check_Relevance\",\n",
        "    relevance_decision,\n",
        "    {\n",
        "        \"Yes\": \"Augment\",\n",
        "        \"No\": \"Web_Search\",\n",
        "    }\n",
        ")\n",
        "\n",
        "# Augment → Generate → END\n",
        "workflow.add_edge(\"Augment\", \"Generate\")\n",
        "workflow.add_edge(\"Generate\", END)\n",
        "\n",
        "print(\"Sample state shape before run:\")\n",
        "print(input_state.keys())\n",
        "\n",
        "# Compile the graph\n",
        "agentic_rag = workflow.compile()\n"
      ],
      "metadata": {
        "id": "PUEjTW6NB8tZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "# Mermaid PNG visualization\n",
        "png_bytes = agentic_rag.get_graph().draw_mermaid_png()\n",
        "display(Image(png_bytes))\n"
      ],
      "metadata": {
        "id": "EMBhyFm3DplF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_state = {\n",
        "    \"query\": \"What is the treatment for cancer?\",\n",
        "    \"context\": \"\",\n",
        "    \"promt\": \"\",\n",
        "    \"response\": \"\",\n",
        "    \"source\": \"\",\n",
        "    \"is_relevant\": \"\",\n",
        "    \"iteration_count\": 0\n",
        "}"
      ],
      "metadata": {
        "id": "jdhb9RE1CDnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "for step in agentic_rag.stream(input_state):\n",
        "    for node_name, state_value in step.items():\n",
        "        print(f\"Finished running: {node_name}\")\n",
        "\n",
        "pprint(state_value[\"response\"])\n"
      ],
      "metadata": {
        "id": "Q9lH9_ZBCPld"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}